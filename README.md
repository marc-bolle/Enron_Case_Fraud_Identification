<p align="center">
  <img src="https://www.ft.com/__origami/service/image/v2/images/raw/https%3A%2F%2Fd1e00ek4ebabms.cloudfront.net%2Fproduction%2F27b002c8-ed2f-41ce-9c00-b09075479e93.jpg?fit=scale-down&source=next&width=700">
</p>

<h1>Machine Learning to Identify Fraud in the Enron Corpus</h1>

<h2>Context</h2>
<p>Enron was an American energy company formed in 1985. It filed for bankruptcy in late 2001 after one of the largest financial scandals in corporate history. Some executive and employees hide billions of dollars in debt from failed deals and projects. Employees were constantly trying to start deals, often regarding the quality of cash flow or profits, as a result of a dysfunctional corporate culture. Many executives were indicted for a variety of charges and some were later sentenced to prison.</p>

<h2>Dataset</h2>
<p>After the company's collapse, over 600,000 emails generated by 158 Enron employees - now known as the Enron Corpus - were acquired by the Federal Energy Regulatory Commission during its investigation and made public. Financial data of top employees and executives were also later released following the trial. The data was uploaded online and a number of people and organizations have graciously prepared, cleaned and organized the dataset to make it available to the public.<br> 
Today, the Enron Corpus is the largest and one of the only publicly available mass collections of real emails easily accessible for study.</p>

<p>Our dataset contains import aspects of the employees' financial data as well as the amount of email exchanged between them.<br>
It contains: 
<ul><li><strong>14 financial features</strong>: salary, bonus, etc.</li> 
  <li><strong>6 email features</strong>: to and from emails with person of interests,</li>
  <li><strong>1 target variable</strong>: boolean labeling whether or not a person is a person of interest (POI).</li></ul></p> 

<h2>Project Goal</h2>
<p>The aim of this project is to develop a machine learning model that will identify Enron employees who may have committed fraud based on their financial and email data.<br> 
The model will be a classifier, that is, a model that will predict the class (POI or not POI) of the data points (employees).<br> 
I will explore the data, replace the missing values, remove the outliers and develop a classifier. The objective is to get a precision and recall scores above 0.42.</p>

<p>This project was the final project of the course Python for Data Science as part of my MSc Artificial Intelligence and Business Analytics at TBS Education. It also used to be the final project of the Udacity Machine Learning course.</p>

<p>- The project was carried out in Python in a Jupyter Notebook file (<strong>code/Enron_Classifier_Notebook.ipynb</strong>) with comments.<br>
- This document explains my reasoning, displays the main lines of code as well as the output</p>

<br>
<hr>
<br>

<h2>Outline</h2>

<h3>Task 1: Explore the data</h3>
<p>1.1 Data exploration<br>
1.2 Replace missing values<br>
1.2.1 Replace missing values of the financial features by zeros<br>
1.2.2 Replace missing values of the salary feature by the mean<br>
1.2.3 Replace missing values of the email features using KNN imputer</p>

<h3>Task 2: Remove outliers</h3>
<p>2.1 Visualize outliers<br>
2.2 Identify outliers using Z-score<br>
2.3 Remove outliers</p>

<h3>Task 3: Create new features</h3>
<p>3.1 Create new calculated features<br>
3.2 Select best features with SelectKBest</p>

<h3>Task 4: Try a variety of classifiers and choose the best one</h3>

<h3>Task 5: Tune the classifier to achieve better than .42 precision and recall</h3>
<p>5.1 Split the dataset into training and test sets<br>
5.2 Find the best parameters with GridSearch<br>
5.3 Fit the model</p>

<h3>Task 6: Dump and evaluate the classifier</h3>

<h3>Conclusion</h3>

<br>
<hr>
<br>

<h2>Task 1: Select what features we will use</h2>
<h3>1.1 Data Exploration</h3>
<p>Features of this dataset fall into three major types:<br>
- Person of Interest (POI) label,<br>
- Financial features (salary, bonus, stocks, etc.),<br>
- Email features (communication between POI and other employees)</p>

<p>Total number of features: 21<br>
Total number of data points: 146<br>
Number of Persons of Interest: 18<br>
Number of people without Person of Interest label: 128</p>

<h3>1.2 Replace Missing Values</h3>
<p align="center">
  <img src="https://github.com/marc-bolle/Enron_Case_Fraud_Identification/blob/main/tools/images/missing_values.jpg">
</p>
<p>The data visualization process reveals that there are lots of missing values in our dataset.<br> 
There are even 5 features that have more than 100 missing data points (out of 146).</p> 
 
<p>The features that contain the highest number of missing values are the financial features (<i>loan_advances, director_fees, restricted_stock_deferred, derral_payments, deferred_income</i>, etc.)<br>
  The variable <i>email_address</i> displays the email address of each employee and has no value to train a machine learning model. Hence, we drop this column from the dataset.<br>   
If I remove all the rows that contain missing values, the dataset would become much too small to train an accurate classifier model. Thus, I will try to impute these missing values.</p>

<h3>1.2.1 Replace missing values of the financial features by zeros</h3>
<p>For the financial features in the <i>financial_features_list</i>, we can assume that the NaN values are zeros. For instance, an employee who do not receive director fees nor a bonus shouldn't have a NaN value but a 0. Thus, I will replace the NaN values in the financial features by zeros.</p>

<p align="center">
  <img src="https://github.com/marc-bolle/Enron_Case_Fraud_Identification/blob/main/tools/images/replace_missing_values.jpg">
</p>
<p>There is one exception for the salary feature where missing values can’t be zeros.</p> 

<h3>1.2.2 Replace missing values of the salary feature by the mean</h3>
<p>There are 51 missing values in the <i>salary</i> feature.<br> 
A missing salary can't be a 0 since each employee should be paid a wage. We can assume that NaN values in the salary variable are true missing values.<br> 
I choose to replace the missing salaries by the mean of all the salaries: $267,102</p>

<p align="center">
  <img src="https://github.com/marc-bolle/Enron_Case_Fraud_Identification/blob/main/tools/images/replace_salary_missing_values.jpg">
</p>
 
<h3>1.2.3 Replace missing values of the email features using KNN imputer</h3>
<p>The email features are very important because they show the degree of relationship between POI and non-POI employees. Replacing these values by the mean or the median or the most common value of the given columns doesn't appear as an effective solution in this case.<br>
I choose to impute these missing values using Scikit Learn's KNNImputer function. It uses the K-nearest Neighbors algorithm to impute the missing values using the mean value of the nearest neighbors found in the dataset.</p>

<p align="center">
  <img src="https://github.com/marc-bolle/Enron_Case_Fraud_Identification/blob/main/tools/images/KNN_imputation_email_missing_values.jpg">
</p>
 
<h2>Task 2: Remove Outliers</h2>
<p>First of all, I remove the <i>‘total row’</i> value since it distorts our data and it is not needed for our machine learning model.</p> 

<h3>2.1 Visualize outliers</h3>
<p>We can plot a boxplot for each feature in the dataset to have a look on outliers.</p>

<p align="center">
  <img src="https://github.com/marc-bolle/Enron_Case_Fraud_Identification/blob/main/tools/images/outliers_boxplot.jpg">
</p>

<p>We can see that there are lots of outliers in our dataset.<br> 
We can also identify the variables that contain very big outliers: <i>restricted_stock, exercised_stock_options, total_stock_value, bonus, loan_advances, total_payments</i>.</p> 

<p>Let’s have a closer look to the six features that have the biggest outliers:</p>

<p align="center">
  <img src="https://github.com/marc-bolle/Enron_Case_Fraud_Identification/blob/main/tools/images/outliers_6_features.jpg">
</p>
 
<h3>2.2 Identify outliers using Z-score</h3>
<p>I decide to use the Z-score to identify the outliers. The Z-score is the number of standard deviations away from the mean that a certain data point is.<br> 
Since the dataset is very small and since there are big outliers, I decide to classify as outlier anybody that has a value (in any feature) superior to a Z-score of 6. That is, six standard deviations away from the mean of the feature.</p>

<p align="center">
  <img src="https://github.com/marc-bolle/Enron_Case_Fraud_Identification/blob/main/tools/images/outliers_z_score.jpg">
</p>

<p>This allows me to identify 10 people. I keep the people who are labeled as Person of Interest (POI) since this is the target variable and this information will be very precious to build the machine learning model.</p>
 
<p>We now have 9 outliers (not POI) that we can remove from the dataset:<br> 
- LOVORATO JOHN J,<br> 
- SHAPIRO RICHARD S,<br> 
- KAMINSKI WINCENTY J,<br> 
- WHITE JR THOMAS E,<br> 
- BHATNAGAR SANJAY,<br> 
- FREVERT MARK A,<br> 
- MARTIN AMANDA K, <br>
- LOCKHART EUGENE E,<br> 
- THE TRAVEL AGENCY IN THE PARK.</p>

<p>After having removed the 10 outliers, we can plot again the boxplots of the outliers:</p>

<p align="center">
  <img src="https://github.com/marc-bolle/Enron_Case_Fraud_Identification/blob/main/tools/images/outliers_boxplot_after.jpg">
</p>

<p align="center">
  <img src="https://github.com/marc-bolle/Enron_Case_Fraud_Identification/blob/main/tools/images/outliers_6_features_after.jpg">
</p>

<p>As we can see, the biggest outliers have been removed but there are still some of them. However, since the dataset is very small, we can’t afford to remove more as too much information for the model will be lost (the dataset is now be composed of 135 employees).</p>

<h2>Task 3: Create new features</h2>
<p>I have to create new features in order to improve the accuracy of the machine learning model.</p> 

<h3>3.1 Create new calculated features</h3>
<p>I use the function compute_fraction to create new features:</p>

<p align="center">
  <img src="https://github.com/marc-bolle/Enron_Case_Fraud_Identification/blob/main/tools/images/function_new_features.jpg">
</p>
 
<p>I’ve decided to create 10 new features:<br>
<i>- fraction_from_poi = from_poi_to_this_person / to_messages<br>
- fraction_to_poi = from_this_person_to_poi / from_messages<br>
- fraction_shared_poi = shared_receipt_with_poi / to_messages<br>
- bonus_to_salary = bonus / salary<br>
- bonus_to_total = bonus / total_payments<br>
- exercised_stock_options_fraction =  exercised_stock_options / total_stock_value<br>
- perc_salary = salary / total_payments<br>
- total_messages = to_messages + from_messages<br>
- total_messages_with_poi = from_this_person_to_poi + from_poi_to_this_person + shared_receipt_with_poi<br>
- message_shared_fraction = total_messages_with_poi / total_messages</i></p>

<p>There are now 30 features in our data set.</p>

<h3>3.2 Select best features with SelectKBest</h3>
<p>I have to choose the best features for the model. The best features are the ones that explain the most the target variable (POI).<br>
I decide to use Scikit learn’s SelectKbest to select the best features and decide to retain the first 5 features with the highest scores (among the 30 variables).</p>

<p align="center">
  <img src="https://github.com/marc-bolle/Enron_Case_Fraud_Identification/blob/main/tools/images/k_best_features.jpg">
</p>

<p>SelectKBest determines that the following features are the best ones: <i>'bonus_to_total', 'deferred_income', 'bonus', 'fraction_shared_poi', 'message_shared_fraction'</i></p>

<p>Then, I extract the features specified in a new list (<i>my_features_list</i>) and we split the data into labels and features.<br>
I then scale the features using Scikit Learn’s MinMaxScaler(), which is a requirement to develop the classifier model.</p>
 
<p>At this point I have selected the features and I would like to try a variety of classifiers to find the one that best suits the data.</p> 

<h2>Task 4 : Try a variety of classifiers</h2>

<p>I test 6 classifiers from the Scikit-Learn package:<br> 
- Gaussian Naive Bayes Classifier<br> 
- Logistic Regression Classifier<br> 
- K-means Clustering<br> 
- Support Vector Machine Classifier<br> 
- Random Forest<br> 
- Gradient Boosting Classifier</p>

<p>The evaluate_clf function split the dataset into training and test sets, fits each classifier to the training set and evaluates the model on the test set with the precision and recall metrics.</p>

<p>The result is the following (they may differ if you run the function again):</p>
<p>Gaussian Naive Bayes Classifier:<br>
- precision: 0.3863454129204129<br>
- recall: 0.30116984126984125</p>
<p>Logistic Regression Classifier:<br>
- precision: 0.46254773004773003<br>
- recall: 0.46254773004773003</p>
<p>K-means Clustering:<br>
- precision: 0.10118862392426371<br>
- recall: 0.47036666666666666</p>
<p>Support Vector Machine Classifier: <br>
- precision: 0.23324965508865833<br>
- recall: 0.8481301587301587</p>
<p>Random Forest: <br>
- precision: 0.4475146825396826<br>
- recall: 0.2437674603174603</p>
<p>Gradient Boosting Classifier: <br>
- precision: 0.3934524531024531<br>
- recall:    0.31042619047619047</p>
<p>The results show us that the Logistic Regression has overall the highest precision and recall metrics. Thus, I will choose this classifier.</p> 

<h2>Task 5: Tune your classifier to achieve better than .42 precision and recall</h2>
<h3>5.1 Split the dataset into training and test sets</h3>
<p>I split the dataset into a training set (70% of the data) and a test set (30% of the data).</p>

<p align="center">
  <img src="https://github.com/marc-bolle/Enron_Case_Fraud_Identification/blob/main/tools/images/train_test_split.jpg">
</p>
 
<h3>5.2 Find the best parameters with GridSearch</h3>
<p>To find the best parameters for logistic regression, I perform a multi-metric evaluation on cross_val_score and GridSearchCV.<br> 
The Stratified K-Folds cross-validator is used for the cross-validation strategy with 10 folds.<br>
The chosen scorers are the following ones: precision_score, recall_score, accuracy_score and f1_score.<br> 
Ultimately, I choose the f1_score to refit the estimator with the parameters setting that has the best cross-validated f1_score.</p> 
 
<p align="center">
  <img src="https://github.com/marc-bolle/Enron_Case_Fraud_Identification/blob/main/tools/images/grid_search_cv.jpg">
</p>
 
<p>The results are the following:</p>

<p align="center">
  <img src="https://github.com/marc-bolle/Enron_Case_Fraud_Identification/blob/main/tools/images/parameters_gridsearchcv.jpg">
</p>
 
<p>As we can see, there is a high number of true negative (TN).</p> 

<h3>5.3 Fit the model</h3>
<p>I can now fit the logistic regression with the best parameters found by GridSearchCV.</p> 

<p align="center">
  <img src="https://github.com/marc-bolle/Enron_Case_Fraud_Identification/blob/main/tools/images/fit_model.jpg">
</p>

<h2>Task 6: Dump and evaluate the classifier</h2> 
<p>I run the tester.py script to evaluate the model. The results are the following:</p>

<p align="center">
  <img src="https://github.com/marc-bolle/Enron_Case_Fraud_Identification/blob/main/tools/images/model_evaluation.jpg">
</p>
 
<h2>Conclusion</h2>
<p>SelectKBest has identified 5 features that are the most meaningful ones to predict whether or not an employee is POI: <i>'bonus_to_total', 'deferred_income', 'bonus', 'fraction_shared_poi' and 'message_shared_fraction'</i>.</p>
<p>I have chosen the logistic regression classifier since it has the highest precision and recall scores.</p>
<p>After performing a multi-metric evaluation on cross_val_score and GridSearchCV to find the best parameters, I fitted the logistic regression on the data. It has an <strong>accuracy score of 0.82</strong>, a <strong>precision of 0.42</strong>, a <strong>recall of 0.66</strong> and a <strong>f1 score of 0.59</strong>.</p> 
<p>Even though these scores seem low, they are actually high considering the very small size of the dataset. This model can roughly predict whether an employee is suspicious or not. Nevertheless, there are undoubtedly improvements to be made. <strong>If you have any comments or suggestions, please let me know.</strong></p>

